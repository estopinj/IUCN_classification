log_features <- function(feat_list, sep=" "){
L <- list()
for (f in feat_list){L <- append(L, colnames(f)[-1])}
return(paste(L, sep=sep))
}
exp_name     <- function(D, sep="_", plote=FALSE, data=FALSE){
set.seed(seed=NULL)
name <- paste0(D[["classifier"]], as.character(sample(1:1000, 1)))
if (plote){
name <- D[["classifier"]]
}
if (data){
name <- paste(paste0(D[["features_names"]], collapse="\n"), name, sep=sep)
}
if (D[["classifier"]]=="MlpIucnn"){
name <- paste(name,
gsub("_", ".", D[["hidden_layers"]]),
sep=sep)
} else if (D[["classifier"]]=="RF"){
name <- paste(name,
paste0("ntree",D[["ntree"]]),
paste0("mtry",D["mtry"]),
sep=sep)
} else if (D[["classifier"]]=="SGD"){
name <- paste(name,
paste(paste0("it" , D[["SGD_max_iter"]]), paste0("estop" , D[["SGD_early_stopping"]]), sep="-"),
paste(D[["grid_params"]][["loss"]], collapse="-"),
paste0("a", paste(D[["grid_params"]][["alpha"]]  , collapse="-")),
paste(D[["grid_params"]][["penalty"]], collapse="-"),
sep=sep)
name <- gsub("modified_huber", "Mhub", name)
name <- gsub("squared_hinge", "hing2", name)
name <- gsub("perceptron", "perc", name)
name <- gsub("FALSE", "Off", name)
name <- gsub("TRUE", "On", name)
}
return(name)
}
####### PREDICTIVE FEATURES & LABELS #######
# IUCN Features
data("training_occ") #geographic occurrences of species with IUCN assessment
# All
iucnn_all <- iucnn_prepare_features(training_occ)   # Training features
# Geo
iucnn_geo <- iucnn_geography_features(x = training_occ)
# HF
iucnn_hf  <- iucnn_footprint_features(x = training_occ)
# Weights
weights64 <- read.csv("/home/jestopin/PHD/data/export_weights/species_weights_s64.csv", header=TRUE, sep=";")
weights32 <- read.csv("/home/jestopin/PHD/data/export_weights/species_weights_s32.csv", header=TRUE, sep=";")
weights64N<- read.csv("/home/jestopin/PHD/data/export_weights/new_model/fc_norm_weight_ALL_b128.csv", header=TRUE, sep=";")
# Test activations
test_avg_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/test_avg_activations.csv", header=TRUE, sep=";")
test_std_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/test_std_activations.csv", header=TRUE, sep=";")
test_avg_std_activ <- read.csv("/home/jestopin/PHD/data/output/activations/test_avg-std_activations.csv", header=TRUE, sep=";")
# Train activations
train_avg_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/train_avg_activations.csv", header=TRUE, sep=";")
train_std_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/train_std_activations.csv", header=TRUE, sep=";")
train_avg_std_activ <- read.csv("/home/jestopin/PHD/data/output/activations/train_avg-std_activations.csv", header=TRUE, sep=";")
# All activations
all_avg_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/all_occs_avg_activations.csv", header=TRUE, sep=";")
all_std_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/all_occs_std_activations.csv", header=TRUE, sep=";")
all_avg_std_activ <- read.csv("/home/jestopin/PHD/data/output/activations/all_occs_avg-std_activations.csv", header=TRUE, sep=";")
# Summed activations
all_sum_activ     <- read.csv("/home/jestopin/PHD/data/output/activations/all_sum_activations.csv", header=TRUE, sep=";")
all_3var_activ    <- read.csv("/home/jestopin/PHD/data/output/activations/all_3var_activations.csv", header=TRUE, sep=";")
# Sum + std activations
sum_std_activ <- all_3var_activ %>% select(-contains("_avg"))
# #Occurrences per species
# occs_count <- training_occ %>% group_by(species) %>% summarise(count = n())
# occs_count <- occs_count %>% arrange(desc(count))
occs_count <- iucnn_geo %>% select(species, tot_occ, uni_occ)
# List containing features & names
D_features <- list("iucnn_all"=iucnn_all,
"iucnn_geo"=iucnn_geo,
"iucnn_hf" =iucnn_hf,
"weights32"=weights32,
"weights64"=weights64,
"occs_count"=occs_count,
"weights64N"=weights64N,
"train_avg_activ"=train_avg_activ,
"train_std_activ"=train_std_activ,
"train_avg_std_activ"=train_avg_std_activ,
"all_avg_activ"=all_avg_activ,
"all_std_activ"=all_std_activ,
"all_avg_std_activ"=all_avg_std_activ,
"test_avg_activ"=test_avg_activ,
"test_std_activ"=test_std_activ,
"test_avg_std_activ"=test_avg_std_activ,
"all_sum_activ"=all_sum_activ,
"all_3var_activ"=all_3var_activ,
"sum_std_activ"=sum_std_activ)
## LABELS ##
data("training_labels")
# labels from IUCN assessed species without weights associated:  3
unknwn_sp <- training_labels %>% filter(species %!in% weights32$species)
# labels from IUCN assessed species not in the TEST set: 572!
unknwn_sp_test <- training_labels %>% filter(species %!in% test_avg_activ$species)
# labels from IUCN assessed species not in the TRAIN set: 17!
unknwn_sp_train <- training_labels %>% filter(species %!in% train_avg_activ$species)
# labels from IUCN assessed species not in ALL OCCS: 17!
unknwn_sp_all <- training_labels %>% filter(species %!in% all_avg_activ$species)
# Common species support between activations & iucnn features: 872
common_sp <- training_labels %>% filter(species %in% all_avg_activ$species)
####### PARAMETERS #######
# load("/home/jestopin/PHD/cactus/R_scripts/.RData")
D <- list()
## seed:
D[["seed"]] <- 1234L
## -- Features:      iucnn_all, iucnn_geo, iucnn_hf, weights32, weights64, occs_count, weights64N, avg_activ, std_activ, avg_std_activ
D[["features_names"]] <- c("all_3var_activ")
## --
# Auto
features_list   <- D_features[D[["features_names"]]]
D[["features"]] <- log_features(features_list)
## Level:         "broad" or "detail"
D["level"] <- "broad"
# test/cv folds:
D["kfold"]  <- 10
D["cvfold"] <- 5L
# TODO: ADD SVM / sgd-classifier
## Classifier:    "MlpIucnn", "RF", "LogLinear", "SGD"
D["classifier"] <- "RF"
## Classifier parameters:
D["verbose"]       <- TRUE
D["hidden_layers"] <- "9_9_9"       # MLP hidden_layers: "N_M_L" format
D["dropout_rate"]  <- 0.1
D["ntree"] <- 10000
D["mtry"]  <- 100 #floor(sqrt(ncol(data)))
D[["SGD_early_stopping"]] <- TRUE
D[["SGD_max_iter"]] <- 1000L
# SGD classif CV GRID SEARCH
params <- list()
params[["loss"]]    <- list("hinge", "modified_huber", "perceptron") #  "log", "squared_hinge",
params[["alpha"]]   <- list(0.0001, 0.001, 0.01, 0.1)
params[["penalty"]] <- list("l2", "l1") # , "elasticnet"
D[["grid_params"]]  <- params
## NAME
D[["EXP_NAME"]] <- exp_name(D)
# --- --- --- ---
####### VARIABLES #######
# ExpBatch folder
ExpLevelData <- paste(D[["level"]], paste(D[["features_names"]], collapse="-"), sep="_")
ExpBatch     <- file.path(base_folder, ExpLevelData)
dir.create(ExpBatch, showWarnings = FALSE)
setwd(ExpBatch)
# Final features
ffeatures <- features_list %>% reduce(inner_join, by='species')
# + labels
# ffeatures <- merge(ffeatures, training_labels, by = 'species') %>% drop_na()
ffeatures <- merge(ffeatures, common_sp, by = 'species') %>% drop_na()
# labels <- training_labels %>% filter(species %in% ffeatures$species)
# labels <- labels[match(ffeatures$species, labels$species),]
# Lookup table
if (D[["level"]]=="broad"){
D[["look.nums"]] <- c(0, 1)
D[["look.labs"]] <- c("Not T.", "Threat.")
} else{
D[["look.nums"]] <- c(0, 1, 2, 3, 4)
D[["look.labs"]] <- c("LC", "NT", "VU", "EN", "CR")
}
# factorize test_preds/labels before computing perfs?
if (D[["classifier"]]=="MlpIucnn" ){
D[["fact_labs"]]  <- TRUE
D[["fact_preds"]] <- TRUE
} else if (D[["classifier"]]=="SGD" ){
D[["fact_labs"]]  <- FALSE
D[["fact_preds"]] <- TRUE
} else{
D[["fact_labs"]]  <- FALSE
D[["fact_preds"]] <- FALSE
}
# Dictionary initialization
D[["val.micro"]]  <- list()
D[["test.micro"]] <- list()
D[["test.macro"]] <- list()
D[["test.sens"]]  <- list()
D[["test.spec"]]  <- list()
####### FUNCTIONS #######
# Macro-avg accuracy
computes_macro_avg <- function(labs, preds){
df <- data.frame(labs, preds)
macro.avg <- df %>%
group_by(labs) %>%
summarise(accuracy = sum(labs == preds) / n()) %>%
summarise(macro.avg = mean(accuracy))
return(macro.avg$macro.avg)
}
# Perfs
perfs <- function(labs, preds, D){
L <- list()
# Macro-avg
macro.avg    <- computes_macro_avg(labs, preds)
L[["macro"]] <- macro.avg
# Factorize labels if need be
if (D[["fact_labs"]]){
labs   <- factor(labs, levels = D[["look.nums"]], labels = D[["look.labs"]])
}
if (D[["fact_preds"]]){
if (D[["classifier"]]=="SGD"){
preds <- factor(preds, levels = D[["look.labs"]], labels = D[["look.labs"]])
} else{
preds <- factor(preds, levels = D[["look.nums"]], labels = D[["look.labs"]])
}
}
print(labs)
print(preds)
# Micro-avg and lot more
u <- union(preds, labs)
t <- table(factor(preds, u), factor(labs, u))
c <- confusionMatrix(t)
L[["micro"]] <- c$overall[['Accuracy']]
if (D[["level"]]=="broad"){
L[["sens"]] <- c$byClass[['Sensitivity']]
L[["spec"]] <- c$byClass[['Specificity']]
}
else{ # detail level
L[["sens"]] <- c$byClass[,'Sensitivity']
L[["spec"]] <- c$byClass[,'Specificity']
}
# Verbose
if (D[["verbose"]]){ print(c); print(paste("Macro-avg:", macro.avg))}
return(L)
}
# Prepares labels
prepares_data_labels <- function(ffeatures, fold, D){
# Test --
test_slice  <- ffeatures %>% slice(fold)
test        <- test_slice %>% select(-labels)
test_labels <- test_slice %>% select(c(species,labels))
# Test --
data_slice  <- ffeatures %>% filter(species %!in% test_slice$species)
data        <- data_slice %>% select(-labels)
data_labels <- data_slice %>% select(c(species,labels))
if (D[["classifier"]]=="MlpIucnn"){
data_labels <- iucnn_prepare_labels(x = data_labels, y = data, level = D[["level"]])
test_labels <- iucnn_prepare_labels(x = test_labels, y = test, level  = D[["level"]])
}else if(D[["level"]]=="broad"){
test_labels <- test_labels %>% mutate(labels = ifelse(labels == "VU" | labels == "EN" | labels == "CR", "Threat.", "Not T."))
data_labels <- data_labels %>% mutate(labels = ifelse(labels == "VU" | labels == "EN" | labels == "CR", "Threat.", "Not T."))
}
if (D[["classifier"]]=="RF"  || D[["classifier"]]=="LogLinear" || D[["classifier"]]=="SGD"){
# Merge of data & labels + factor labels
data$species <- NULL
test$species <- NULL
data_labels$labels  <- factor(data_labels$labels, levels = D[["look.labs"]], labels = D[["look.labs"]])
test_labels$labels  <- factor(test_labels$labels, levels = D[["look.labs"]], labels = D[["look.labs"]])
# Type conversion
data <- data.matrix(data)
test <- data.matrix(test)
# Scales data
scaler <- sklearn$preprocessing$StandardScaler()
scaler$fit(data)
data <- scaler$transform(data)
test <- scaler$transform(test)
if (D[["classifier"]]=="RF" || D[["classifier"]]=="LogLinear"){
# Type re-conversion
data <- data.frame(data)
test <- data.frame(test)
}
}
return(list("data"=data, "data_labels"=data_labels, "test"=test, "test_labels"=test_labels))
}
# Classifier switch
classifier_switch <- function(data, data_labels, test, test_labels, D, names, i){
switch(D[['classifier']],
MlpIucnn={
# 5-FOLD CROSS-VALIDATION ---
res_cv <- iucnn_train_model(x   = data,
lab = data_labels,
cv_fold = D[["cvfold"]],
n_layers = D[["hidden_layers"]],
seed = D[['seed']],
balance_classes = FALSE,
dropout_rate = D[["dropout_rate"]],
path_to_output = "models/res_cv",
overwrite = TRUE,
verbose = D[["verbose"]])
val.micro <- res_cv$validation_accuracy
# PROD MODEL trained on train+val during avg_cv_best_epoch ---
res_prod <- iucnn_train_model(x = data,
lab = data_labels,
production_model = res_cv,
seed = D[['seed']],
path_to_output = "models/res_prod",
overwrite = TRUE,
verbose = D[["verbose"]])
# 10% TEST SET ACCURACY ---
test_preds <- iucnn_predict_status(x = test, model = res_prod, return_IUCN = FALSE)$class_predictions
test_perfs <- perfs(test_labels$labels$labels, test_preds, D)
},
RF={
print('RF')
# Train algorithm
RFmodel <- randomForest(x     = data,
y     = data_labels$labels,
ntree = D[["ntree"]],
mtry  = D[["mtry"]],
na.action = na.omit)
val.micro <- -1
# Test set prediction
test_preds <- predict(RFmodel, test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
LogLinear={
# Fitting Multinomial log-linear regression to the Training set
regressor <- multinom(formula = data_labels$labels ~ ., data = data)
val.micro <- -1
# Test set prediction
test_preds <- predict(regressor, test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
SGD={
print("SGD classifier")
SGDClassif <- sklearn$linear_model$SGDClassifier(max_iter=D[["SGD_max_iter"]],
early_stopping=D[["SGD_early_stopping"]],
validation_fraction=1/D[["cvfold"]],
random_state=D[['seed']])
# GRID SEARCH
grid <- sklearn$model_selection$GridSearchCV(SGDClassif,
param_grid=D[["grid_params"]],
cv=D[["cvfold"]],
refit=TRUE)
grid$fit(data, data_labels$labels)
D[["bestGridSearch.params"]][[names[i]]]  <- grid$best_params_
# Best cross-validation score
val.micro <- grid$best_score_
# Test
test_preds <- grid$predict(test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
bar={
# case 'bar' here...
print('bar')
},
{
stop("Enter a valid classifier.") #default
}
)
## Val/Test performances integration
D[["val.micro"]][[names[i]]]  <- val.micro
D[["test.micro"]][[names[i]]] <- test_perfs[["micro"]]
D[["test.macro"]][[names[i]]] <- test_perfs[["macro"]]
D[["test.sens"]][[names[i]]]  <- test_perfs[["sens"]]
D[["test.spec"]][[names[i]]]  <- test_perfs[["spec"]]
return(D)
}
# Avg perfs function
log_avg_perfs <- function(D){
# Validation Micro accuracy
D[["avg.val.micro"]] <- D[["val.micro"]] %>% unlist() %>% mean()
# Test Micro & macro accuracy
D[["avg.test.micro"]] <- D[["test.micro"]] %>% unlist() %>% mean()
D[["avg.test.macro"]] <- D[["test.macro"]] %>% unlist() %>% mean()
# Test Sensibility / Specificity
if (D[["level"]]=="broad"){
D[["avg.test.sens"]] <- D[["test.sens"]] %>% unlist() %>% mean()
D[["avg.test.spec"]] <- D[["test.spec"]] %>% unlist() %>% mean()
} else{ # detail
D[["avg.test.sens"]] <- data.frame(D[["test.sens"]]) %>% apply(1,mean)
D[["avg.test.spec"]] <- data.frame(D[["test.spec"]]) %>% apply(1,mean)
}
return(D)
}
# Log function
log <- function(ExpBatch, D){
# log file name
exp_log <- file.path(ExpBatch, paste(D[["EXP_NAME"]], Sys.time(), sep="_"))
# Content
log <- list()
for (i in seq_along(D)){
log <- append(log, c(names(D)[i], D[[i]], "\n"))
}
# Writing
cat(unlist(log),file=paste0(exp_log,".txt"),sep="\n",append=TRUE)
saveRDS(D, file=paste0(exp_log,".RData"))
}
# Recovers results before models comparison
recovers_batch_perfs <- function(ExpBatch, plote=TRUE){
# Initializes results data structure
res <- list()
res$val.micro  <- list()
res$test.micro <- list()
res$test.macro <- list()
# Recovers val.micro, test.micro, test.macro for exps in ExpBatch
LExp <- Sys.glob(file.path(ExpBatch,'*.RData'))
for (i in seq_along(LExp)) {
Dexp <- readRDS(LExp[[i]])
res$val.micro[[Dexp[["EXP_NAME"]]]]  <- unlist(Dexp$val.micro )
res$test.micro[[Dexp[["EXP_NAME"]]]] <- unlist(Dexp$test.micro )
res$test.macro[[Dexp[["EXP_NAME"]]]] <- unlist(Dexp$test.macro)
}
# res$val.micro  <- list(res$val.micro)
# res$test.micro <- list(res$test.micro)
# res$test.macro <- list(res$test.macro)
return(res)
}
# For a given:
# *level in "detail"/"broad"
# *criterium "test.micro"/"test.macro"
# Recovers model with best avg performance for each data source and compare them
recovers_best_models <- function(base_folder, level, criterium, sep="--", plote=TRUE){
# List of folders matching required level
Lf <- Sys.glob(file.path(base_folder, paste(level,"*",sep="_")))
# Initializes list of best model per folder
Lb_Dexp <- list()
# For loop on folders
for (f in Lf) {
cat("\t\t ***",f,"*** \n")
# Recovers the list of experiments in this folder
LExp <- Sys.glob(file.path(f,'*.RData'))
if (length(LExp)==1){
# Only 1 exp
best_Dexp <- readRDS(LExp[[1]])
best_avg  <- best_Dexp[[paste0("avg.",criterium)]]
Lb_Dexp   <- append(Lb_Dexp, list(best_Dexp))
cat("\t", best_Dexp[["EXP_NAME"]], "avg \t:", best_avg, "\n")
} else if (length(LExp)>1){
# 2 or more exps
best_Dexp <- readRDS(LExp[[1]])
best_avg <- best_Dexp[[paste0("avg.",criterium)]]
for (exp in LExp){
Dexp <- readRDS(exp)
avg  <- Dexp[[paste0("avg.",criterium)]]
cat("\t", Dexp[["EXP_NAME"]], "avg \t:", avg, "\n")
if (avg > best_avg){
best_Dexp <- Dexp
best_avg  <- avg
}
}
Lb_Dexp <- append(Lb_Dexp, list(best_Dexp))
}
cat("\t\t BEST:", best_Dexp[["EXP_NAME"]], "avg \t:", best_avg, "\n\n")
}
print(paste("Nb retained exps:", length(Lb_Dexp)))
res <- list()
for (Dexp in Lb_Dexp){
name <- paste(paste0(Dexp[["features_names"]], collapse="\n"),
exp_name(Dexp, plote=TRUE, sep=sep), #Dexp[["EXP_NAME"]], # TODO: CHANGE here
sep=sep)
res[[name]]  <- unlist(Dexp[[criterium]])
}
# ordering res by best avg
L_avg      <- lapply(res, mean)
sort_index <- order(as.double(L_avg), decreasing = TRUE) # CHANGED here
print(sort_index)
return(res[sort_index])
}
# Plots best models per data source to compare them
plots_best_results <- function(base_folder, lvl, crit){
res  <- recovers_best_models(base_folder, lvl, crit, sep="--")
if (lvl=="detail"){
if (crit=="test.micro"){
ylim <- c(0.22, 0.72)
} else {
ylim <- c(0.13, 0.53)
}
} else {
ylim <- c(0.35, 0.90)
}
# Vertical box plot by group
names(res) <- gsub("--", "\n", names(res))
print(names(res))
names(res) <- c("IUCNN ref., RF*\nGeo. + HF features", "RF\nSDM avg/std/sum activations")
print(names(res))
# names(res) <- gsub("-", "\n", names(res))
boxplot(res, col = "white", ylim = ylim)
# Points  c(3,2),
stripchart(res, method = "jitter", pch = 19, col =  2:(length(res)+1),
vertical = TRUE, add = TRUE)
# Constructs averages
means <- as.double(lapply(res, mean))
names(means) <- names(res)
points(means, col = "black", pch = 18, cex=1.75)
title(paste("Crit :  ", crit,
"\nLevel   :  ", lvl))
par(mgp=c(3,-1.5,0))
grid(nx = NA, ny = NULL)
}
data("training_labels")
View(training_labels)
hist(training_labels)
hist(training_labels$labels)
plot(training_labels$labels)
iucnn_prepare_labels(training_labels$labels)
iucnn_prepare_labels(x = training_labels,
y = training_labels,
level = "detail") # Training labels
plot(toplot)
toplot <- iucnn_prepare_labels(x = training_labels,
y = training_labels,
level = "detail") # Training labels
plot(toplot)
plot(toplot$labels)
plot(toplot$labels)
toplot <- iucnn_prepare_labels(x = training_labels,
y = training_labels,
level = "detail") # Training labels
plot(toplot$labels)
# Plot statuses
# 1. Feature and label preparation
features <- iucnn_prepare_features(training_occ) # Training features
labels_train <- iucnn_prepare_labels(x = training_labels,
y = features) # Training labels
# 2. Model training
m1 <- iucnn_train_model(x = features, lab = labels_train)
summary(m1)
# 2. Model training
m1 <- iucnn_train_model(x = features, lab = labels_train, overwrite = TRUE)
summary(m1)
plot(m1)
hist(labels_train)
