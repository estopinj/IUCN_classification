####### PARAMETERS #######
# load("/home/jestopin/PHD/cactus/R_scripts/.RData")
D <- list()
## seed:
D[["seed"]] <- 1234L
## -- Features:      iucnn_all, iucnn_geo, iucnn_hf, weights32, weights64, occs_count, weights64N, avg_activ, std_activ, avg_std_activ
D[["features_names"]] <- c("iucnn_geo")
## --
# Auto
features_list   <- D_features[D[["features_names"]]]
D[["features"]] <- log_features(features_list)
## Level:         "broad" or "detail"
D["level"] <- "broad"
# test/cv folds:
D["kfold"]  <- 10
D["cvfold"] <- 5L
# TODO: ADD SVM / sgd-classifier
## Classifier:    "MlpIucnn", "RF", "LogLinear", "SGD"
D["classifier"] <- "MlpIucnn"
## Classifier parameters:
D["verbose"]       <- TRUE
D["hidden_layers"] <- "9_9_9"       # MLP hidden_layers: "N_M_L" format
D["ntree"] <- 10000
D["mtry"]  <- 50 #floor(sqrt(ncol(data)))
D[["SGD_early_stopping"]] <- TRUE
D[["SGD_max_iter"]] <- 1000L
# SGD classif CV GRID SEARCH
params <- list()
params[["loss"]]    <- list("hinge", "modified_huber", "perceptron") #  "log", "squared_hinge",
params[["alpha"]]   <- list(0.0001, 0.001, 0.01, 0.1)
params[["penalty"]] <- list("l2", "l1") # , "elasticnet"
D[["grid_params"]]  <- params
## NAME
D[["EXP_NAME"]] <- exp_name(D)
# --- --- --- ---
####### VARIABLES #######
# ExpBatch folder
ExpLevelData <- paste(D[["level"]], paste(D[["features_names"]], collapse="-"), sep="_")
ExpBatch     <- file.path(base_folder, ExpLevelData)
dir.create(ExpBatch, showWarnings = FALSE)
setwd(ExpBatch)
# Final features
ffeatures <- features_list %>% reduce(inner_join, by='species')
# + labels
ffeatures <- merge(ffeatures, training_labels, by = 'species') %>% drop_na()
# labels <- training_labels %>% filter(species %in% ffeatures$species)
# labels <- labels[match(ffeatures$species, labels$species),]
# Lookup table
if (D[["level"]]=="broad"){
D[["look.nums"]] <- c(0, 1)
D[["look.labs"]] <- c("Not T.", "Threat.")
} else{
D[["look.nums"]] <- c(0, 1, 2, 3, 4)
D[["look.labs"]] <- c("LC", "NT", "VU", "EN", "CR")
}
# factorize test_preds/labels before computing perfs?
if (D[["classifier"]]=="MlpIucnn" ){
D[["fact_labs"]]  <- TRUE
D[["fact_preds"]] <- TRUE
} else if (D[["classifier"]]=="SGD" ){
D[["fact_labs"]]  <- FALSE
D[["fact_preds"]] <- TRUE
} else{
D[["fact_labs"]]  <- FALSE
D[["fact_preds"]] <- FALSE
}
# Dictionary initialization
D[["val.micro"]]  <- list()
D[["test.micro"]] <- list()
D[["test.macro"]] <- list()
D[["test.sens"]]  <- list()
D[["test.spec"]]  <- list()
####### FUNCTIONS #######
# Macro-avg accuracy
computes_macro_avg <- function(labs, preds){
df <- data.frame(labs, preds)
macro.avg <- df %>%
group_by(labs) %>%
summarise(accuracy = sum(labs == preds) / n()) %>%
summarise(macro.avg = mean(accuracy))
return(macro.avg$macro.avg)
}
# Perfs
perfs <- function(labs, preds, D){
L <- list()
# Macro-avg
macro.avg    <- computes_macro_avg(labs, preds)
L[["macro"]] <- macro.avg
# Factorize labels if need be
if (D[["fact_labs"]]){
labs   <- factor(labs, levels = D[["look.nums"]], labels = D[["look.labs"]])
}
if (D[["fact_preds"]]){
if (D[["classifier"]]=="SGD"){
preds <- factor(preds, levels = D[["look.labs"]], labels = D[["look.labs"]])
} else{
preds <- factor(preds, levels = D[["look.nums"]], labels = D[["look.labs"]])
}
}
print(labs)
print(preds)
# Micro-avg and lot more
u <- union(preds, labs)
t <- table(factor(preds, u), factor(labs, u))
c <- confusionMatrix(t)
L[["micro"]] <- c$overall[['Accuracy']]
if (D[["level"]]=="broad"){
L[["sens"]] <- c$byClass[['Sensitivity']]
L[["spec"]] <- c$byClass[['Specificity']]
}
else{ # detail level
L[["sens"]] <- c$byClass[,'Sensitivity']
L[["spec"]] <- c$byClass[,'Specificity']
}
# Verbose
if (D[["verbose"]]){ print(c); print(paste("Macro-avg:", macro.avg))}
return(L)
}
# Prepares labels
prepares_data_labels <- function(ffeatures, fold, D){
# Test --
test_slice  <- ffeatures %>% slice(fold)
test        <- test_slice %>% select(-labels)
test_labels <- test_slice %>% select(c(species,labels))
# Test --
data_slice  <- ffeatures %>% filter(species %!in% test_slice$species)
data        <- data_slice %>% select(-labels)
data_labels <- data_slice %>% select(c(species,labels))
if (D[["classifier"]]=="MlpIucnn"){
data_labels <- iucnn_prepare_labels(x = data_labels, y = data, level = D[["level"]])
test_labels <- iucnn_prepare_labels(x = test_labels, y = test, level  = D[["level"]])
}else if(D[["level"]]=="broad"){
test_labels <- test_labels %>% mutate(labels = ifelse(labels == "VU" | labels == "EN" | labels == "CR", "Threat.", "Not T."))
data_labels <- data_labels %>% mutate(labels = ifelse(labels == "VU" | labels == "EN" | labels == "CR", "Threat.", "Not T."))
}
if (D[["classifier"]]=="RF"  || D[["classifier"]]=="LogLinear" || D[["classifier"]]=="SGD"){
# Merge of data & labels + factor labels
data$species <- NULL
test$species <- NULL
data_labels$labels  <- factor(data_labels$labels, levels = D[["look.labs"]], labels = D[["look.labs"]])
test_labels$labels  <- factor(test_labels$labels, levels = D[["look.labs"]], labels = D[["look.labs"]])
# Type conversion
data <- data.matrix(data)
test <- data.matrix(test)
# Scales data
scaler <- sklearn$preprocessing$StandardScaler()
scaler$fit(data)
data <- scaler$transform(data)
test <- scaler$transform(test)
if (D[["classifier"]]=="RF" || D[["classifier"]]=="LogLinear"){
# Type re-conversion
data <- data.frame(data)
test <- data.frame(test)
}
}
return(list("data"=data, "data_labels"=data_labels, "test"=test, "test_labels"=test_labels))
}
# Classifier switch
classifier_switch <- function(data, data_labels, test, test_labels, D, names, i){
switch(D[['classifier']],
MlpIucnn={
# 5-FOLD CROSS-VALIDATION ---
res_cv <- iucnn_train_model(x   = data,
lab = data_labels,
cv_fold = D[["cvfold"]],
n_layers = D[["hidden_layers"]],
seed = D[['seed']],
balance_classes = FALSE,
path_to_output = "models/res_cv",
overwrite = TRUE,
verbose = D[["verbose"]])
val.micro <- res_cv$validation_accuracy
# PROD MODEL trained on train+val during avg_cv_best_epoch ---
res_prod <- iucnn_train_model(x = data,
lab = data_labels,
production_model = res_cv,
seed = D[['seed']],
path_to_output = "models/res_prod",
overwrite = TRUE,
verbose = D[["verbose"]])
# 10% TEST SET ACCURACY ---
test_preds <- iucnn_predict_status(x = test, model = res_prod, return_IUCN = FALSE)$class_predictions
test_perfs <- perfs(test_labels$labels$labels, test_preds, D)
},
RF={
print('RF')
# Train algorithm
RFmodel <- randomForest(x     = data,
y     = data_labels$labels,
ntree = D[["ntree"]],
mtry  = D[["mtry"]],
na.action = na.omit)
val.micro <- -1
# Test set prediction
test_preds <- predict(RFmodel, test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
LogLinear={
# Fitting Multinomial log-linear regression to the Training set
regressor <- multinom(formula = data_labels$labels ~ ., data = data)
val.micro <- -1
# Test set prediction
test_preds <- predict(regressor, test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
SGD={
print("SGD classifier")
SGDClassif <- sklearn$linear_model$SGDClassifier(max_iter=D[["SGD_max_iter"]],
early_stopping=D[["SGD_early_stopping"]],
validation_fraction=1/D[["cvfold"]],
random_state=D[['seed']])
# GRID SEARCH
grid <- sklearn$model_selection$GridSearchCV(SGDClassif,
param_grid=D[["grid_params"]],
cv=D[["cvfold"]],
refit=TRUE)
grid$fit(data, data_labels$labels)
D[["bestGridSearch.params"]][[names[i]]]  <- grid$best_params_
# Best cross-validation score
val.micro <- grid$best_score_
# Test
test_preds <- grid$predict(test)
test_perfs <- perfs(test_labels$labels, test_preds, D)
},
bar={
# case 'bar' here...
print('bar')
},
{
stop("Enter a valid classifier.") #default
}
)
## Val/Test performances integration
D[["val.micro"]][[names[i]]]  <- val.micro
D[["test.micro"]][[names[i]]] <- test_perfs[["micro"]]
D[["test.macro"]][[names[i]]] <- test_perfs[["macro"]]
D[["test.sens"]][[names[i]]]  <- test_perfs[["sens"]]
D[["test.spec"]][[names[i]]]  <- test_perfs[["spec"]]
return(D)
}
# Avg perfs function
log_avg_perfs <- function(D){
# Validation Micro accuracy
D[["avg.val.micro"]] <- D[["val.micro"]] %>% unlist() %>% mean()
# Test Micro & macro accuracy
D[["avg.test.micro"]] <- D[["test.micro"]] %>% unlist() %>% mean()
D[["avg.test.macro"]] <- D[["test.macro"]] %>% unlist() %>% mean()
# Test Sensibility / Specificity
if (D[["level"]]=="broad"){
D[["avg.test.sens"]] <- D[["test.sens"]] %>% unlist() %>% mean()
D[["avg.test.spec"]] <- D[["test.spec"]] %>% unlist() %>% mean()
} else{ # detail
D[["avg.test.sens"]] <- data.frame(D[["test.sens"]]) %>% apply(1,mean)
D[["avg.test.spec"]] <- data.frame(D[["test.spec"]]) %>% apply(1,mean)
}
return(D)
}
# Log function
log <- function(ExpBatch, D){
# log file name
exp_log <- file.path(ExpBatch, paste(D[["EXP_NAME"]], Sys.time(), sep="_"))
# Content
log <- list()
for (i in seq_along(D)){
log <- append(log, c(names(D)[i], D[[i]], "\n"))
}
# Writing
cat(unlist(log),file=paste0(exp_log,".txt"),sep="\n",append=TRUE)
saveRDS(D, file=paste0(exp_log,".RData"))
}
# Recovers results before models comparison
recovers_batch_perfs <- function(ExpBatch, plote=TRUE){
# Initializes results data structure
res <- list()
res$val.micro  <- list()
res$test.micro <- list()
res$test.macro <- list()
# Recovers val.micro, test.micro, test.macro for exps in ExpBatch
LExp <- Sys.glob(file.path(ExpBatch,'*.RData'))
for (i in seq_along(LExp)) {
Dexp <- readRDS(LExp[[i]])
res$val.micro[[Dexp[["EXP_NAME"]]]]  <- unlist(Dexp$val.micro )
res$test.micro[[Dexp[["EXP_NAME"]]]] <- unlist(Dexp$test.micro )
res$test.macro[[Dexp[["EXP_NAME"]]]] <- unlist(Dexp$test.macro)
}
# res$val.micro  <- list(res$val.micro)
# res$test.micro <- list(res$test.micro)
# res$test.macro <- list(res$test.macro)
return(res)
}
# For a given:
# *level in "detail"/"broad"
# *criterium "test.micro"/"test.macro"
# Recovers model with best avg performance for each data source and compare them
recovers_best_models <- function(base_folder, level, criterium, sep="--", plote=TRUE){
# List of folders matching required level
Lf <- Sys.glob(file.path(base_folder, paste(level,"*",sep="_")))
# Initializes list of best model per folder
Lb_Dexp <- list()
# For loop on folders
for (f in Lf) {
cat("\t\t ***",f,"*** \n")
# Recovers the list of experiments in this folder
LExp <- Sys.glob(file.path(f,'*.RData'))
if (length(LExp)==1){
# Only 1 exp
best_Dexp <- readRDS(LExp[[1]])
best_avg  <- best_Dexp[[paste0("avg.",criterium)]]
Lb_Dexp   <- append(Lb_Dexp, list(best_Dexp))
cat("\t", best_Dexp[["EXP_NAME"]], "avg \t:", best_avg, "\n")
} else if (length(LExp)>1){
# 2 or more exps
best_Dexp <- readRDS(LExp[[1]])
best_avg <- best_Dexp[[paste0("avg.",criterium)]]
for (exp in LExp){
Dexp <- readRDS(exp)
avg  <- Dexp[[paste0("avg.",criterium)]]
cat("\t", Dexp[["EXP_NAME"]], "avg \t:", avg, "\n")
if (avg > best_avg){
best_Dexp <- Dexp
best_avg  <- avg
}
}
Lb_Dexp <- append(Lb_Dexp, list(best_Dexp))
}
cat("\t\t BEST:", best_Dexp[["EXP_NAME"]], "avg \t:", best_avg, "\n\n")
}
print(paste("Nb retained exps:", length(Lb_Dexp)))
res <- list()
for (Dexp in Lb_Dexp){
name <- paste(paste0(Dexp[["features_names"]], collapse="\n"),
exp_name(Dexp, plote=TRUE, sep=sep), #Dexp[["EXP_NAME"]], # TODO: CHANGE here
sep=sep)
res[[name]]  <- unlist(Dexp[[criterium]])
}
# ordering res by best avg
L_avg      <- lapply(res, mean)
sort_index <- order(as.double(L_avg), decreasing = TRUE)
print(sort_index)
return(res[sort_index])
}
# Plots best models per data source to compare them
plots_best_results <- function(base_folder, lvl, crit){
res  <- recovers_best_models(base_folder, lvl, crit, sep="--")
if (lvl=="detail"){
if (crit=="test.micro"){
ylim <- c(0.22, 0.72)
} else {
ylim <- c(0.13, 0.53)
}
} else {
ylim <- c(0.35, 0.90)
}
# Vertical box plot by group
names(res) <- gsub("--", "\n", names(res))
# names(res) <- gsub("-", "\n", names(res))
boxplot(res, col = "white", ylim = ylim)
# Points
stripchart(res, method = "jitter", pch = 19, col = 2:(length(res)+1),
vertical = TRUE, add = TRUE)
# Constructs averages
means <- as.double(lapply(res, mean))
names(means) <- names(res)
points(means, col = "black", pch = 18, cex=1.75)
title(paste("Crit :  ", crit,
"\nLevel   :  ", lvl))
par(mgp=c(3,-1.5,0))
grid(nx = NA, ny = NULL)
}
####### 10-FOLD TEST #######
# Sets seed for reproducibility
set.seed(D[["seed"]])
folds <- createFolds(1:nrow(ffeatures), k = D[["kfold"]])
for (i in seq_along(folds)) {
print(names(folds)[i])
# Prepares data & labels
split       <- prepares_data_labels(ffeatures, folds[[i]], D)
data        <- split[["data"]]
data_labels <- split[["data_labels"]]
test        <- split[["test"]]
test_labels <- split[["test_labels"]]
### Classifier switch ###
D <- classifier_switch(data, data_labels, test, test_labels, D, names(folds), i)
}
### AVG PERFS ###
D <- log_avg_perfs(D)
### LOG ###
print(D)
log(ExpBatch, D)
View(training_labels)
typeof(training_labels)
labels_df <- data.frame(species = training_labels$species,
lebels = training_labels$labels)
View(labels_df)
TYPEOF(labels_df)
typeof(labels_df)
write.csv(training_labels,
"/home/jestopin/PHD/cactus/R_scripts/exports/IUCN_GroundTruth.csv",
row.names = TRUE)
View(ffeatures)
View(data)
# Labels preparation
labels_train <- iucnn_prepare_labels(x = training_labels,
y = iucnn_geo) # Training labels
mod_test <- iucnn_modeltest(x = iucnn_geo,
lab = labels_train,
mode = "nn-class",
dropout_rate = c(0.0, 0.1, 0.3),
n_layers = c("9_9_9"),
cv_fold = 5)
# Select best model
m_best <- iucnn_best_model(x = mod_test,
criterion = "val_acc",
require_dropout = TRUE)
# Labels preparation
labels_train <- iucnn_prepare_labels(x = training_labels,
y = iucnn_geo,
level = "broad") # Training labels
mod_test <- iucnn_modeltest(x = iucnn_geo,
lab = labels_train,
mode = "nn-class",
dropout_rate = c(0.0, 0.1, 0.3),
n_layers = c("9_9_9"),
cv_fold = 5)
m_best <- iucnn_best_model(x = mod_test,
criterion = "val_acc",
require_dropout = TRUE)
# Labels preparation
labels_train <- iucnn_prepare_labels(x = training_labels,
y = iucnn_geo,
level = "broad") # Training labels
mod_test <- iucnn_modeltest(x = iucnn_geo,
lab = labels_train,
mode = "nn-class",
dropout_rate = c(0.0, 0.1, 0.3),
n_layers = c("9_9_9"),
cv_fold = 5,
model_outpath = "models/")
# Select best model
m_best <- iucnn_best_model(x = mod_test,
criterion = "val_acc",
require_dropout = TRUE)
# Inspect model structure and performance
summary(m_best)
plot(m_best)
# Train the best model on all training data for prediction
m_prod <- iucnn_train_model(x = iucnn_geo,
lab = labels_train,
production_model = m_best)
geo_predict <- iucnn_geography_features(prediction_occ) # Prediction features
# Predict RL categories for target species
pred <- iucnn_predict_status(x = features_predict,
model = m_prod)
# Predict RL categories for target species
pred <- iucnn_predict_status(x = geo_predict,
model = m_prod)
plot(pred)
View(pred)
pred_labels = data.frame(species = pred$names,
labels = pred$class_predictions)
View(pred_labels)
write.csv(training_labels,
"/home/jestopin/PHD/cactus/R_scripts/exports/pred_labels100.csv",
row.names = TRUE)
write.csv(pred_labels,
"/home/jestopin/PHD/cactus/R_scripts/exports/pred_labels100.csv",
row.names = TRUE)
# All occurrences
ref <- "/home/jestopin/PHD/data/occurrences/orchids_extraction_ref/DeepOrchidSeries.csv"
read.csv(file = ref)
# All occurrences
ref    <- "/home/jestopin/PHD/data/occurrences/orchids_extraction_ref/DeepOrchidSeries.csv"
df_ref <- read.csv(file = ref)
View(df_ref)
df_ref <- read.csv(file = ref, sep = ";")
head(carSpeeds)
head(df_ref)
View(df_ref)
View(geo_predict)
View(prediction_occ)
all_occs <- df_ref %>% select(canonical_name, decimallongitude, decimallatitude)
View(all_occs)
names(all_occs)
names(all_occs)[1] <- "species"
# Predict RL categories for target species
pred_all <- iucnn_predict_status(x = all_occs,
model = m_prod)
geo_all_occs       <- iucnn_geography_features(all_occs)
View(prediction_occ)
# Predictions ----
data("prediction_occ") # No ground truth
prediction_occ
# Predict RL categories for target species
pred_all <- iucnn_predict_status(x = all_occs,
model = m_prod)
# Predict RL categories for target species
pred_all <- iucnn_predict_status(x = geo_all_occs,
model = m_prod)
plot(pred_all)
pred_labels <- data.frame(species = pred$names,
labels = pred$class_predictions)
pred_labels <- data.frame(species = pred_all$names,
labels = pred_all$class_predictions)
View(pred_labels)
write.csv(pred_labels_all,
"/home/jestopin/PHD/cactus/R_scripts/exports/pred_labels_all.csv",
row.names = TRUE)
pred_labels_all <- data.frame(species = pred_all$names,
labels = pred_all$class_predictions)
write.csv(pred_labels_all,
"/home/jestopin/PHD/cactus/R_scripts/exports/pred_labels_all.csv",
row.names = TRUE)
